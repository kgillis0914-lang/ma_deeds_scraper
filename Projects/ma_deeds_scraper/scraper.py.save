import os, json
from pathlib import Path
from dotenv import load_dotenv
from playwright.sync_api import sync_playwright
import pandas as pd

load_dotenv()

DATE_FROM = os.getenv("DATE_FROM", "2025-09-01")
DATE_TO = os.getenv("DATE_TO", "2025-09-17")
COUNTIES_FILE = os.getenv("COUNTIES_FILE", "counties.json")
OUT_CSV = os.getenv("OUT_CSV", "ma_deeds_data/records.csv")

def load_counties():
    path = Path(COUNTIES_FILE)
    if not path.exists():
        raise FileNotFoundError(f"Counties config not found: {COUNTIES_FILE}")
    return json.loads(path.read_text())

def ensure_dirs():
    Path("ma_deeds_data").mkdir(parents=True, exist_ok=True)
    Path("downloads").mkdir(parents=True, exist_ok=True)

# -------- Vendor routines --------
def scrape_masslandrecords(page, url, county):
    page.goto(url, timeout=60000)
    for f in ["#RecordedDateFrom", "input[name='RecordedDateFrom']", "#FromDate"]:
        try: page.fill(f, DATE_FROM, timeout=500); break
        except: pass
    for t in ["#RecordedDateTo", "input[name='RecordedDateTo']", "#ToDate"]:
        try: page.fill(t, DATE_TO, timeout=500); break
        except: pass
def scrape_browntech_barnstable(page, url, county):
    page.goto(url, timeout=60000)
    for f in ["input[name='FromDate']", "#FromDate"]:
        try: page.fill(f, DATE_FROM); break
        except: pass
    for t in ["input[name='ToDate']", "#ToDate"]:
        try: page.fill(t, DATE_TO); break
        except: pass
    for btn in ["input[type='submit']", "button:has-text('Search')"]:
        try: page.click(btn); break
        except: pass
    page.wait_for_timeout(1500)
    return [a.get_attribute("href") for a in page.query_selector_all("a[href$='.pdf']")]

def scrape_browntech_alis(page, url, county):
    page.goto(url, timeout=60000)
    for f in ["input[name='RecordedFrom']", "#RecordedFrom"]:
        try: page.fill(f, DATE_FROM); break
        except: pass
    for t in ["input[name='RecordedTo']", "#RecordedTo"]:
        try: page.fill(t, DATE_TO); break
        except: pass
    for btn in ["input[type='submit']", "button:has-text('Search')"]:
        try: page.click(btn); break
        except: pass
    page.wait_for_timeout(1500)
    return [a.get_attribute("href") for a in page.query_selector_all("a[href$='.pdf']")]

def scrape_kofile_titleview(page, url, county):
    page.goto(url, timeout=60000)
    for f in ["input#fromDate", "input[name='fromDate']"]:
        try: page.fill(f, DATE_FROM); break
        except: pass
    for t in ["input#toDate", "input[name='toDate']"]:
        try: page.fill(t, DATE_TO); break
        except: pass
    for btn in ["button:has-text('Search')", "input[type='submit']"]:
        try: page.click(btn); break
        except: pass
    page.wait_for_timeout(1500)
    return [a.get_attribute("href") for a in page.query_selector_all("a[href$='.pdf']")]

def scrape_kofile_taunton(page, url, county):
    page.goto(url, timeout=60000)
    for f in ["input[name='fromDate']", "#fromDate"]:
        try: page.fill(f, DATE_FROM); break
        except: pass
    for t in ["input[name='toDate']", "#toDate"]:
        try: page.fill(t, DATE_TO); break
        except: pass
    for btn in ["button:has-text('Search')", "input[type='submit']"]:
        try: page.click(btn); break
        except: pass
    page.wait_for_timeout(1500)
    return [a.get_attribute("href") for a in page.query_selector_all("a[href$='.pdf']")]

def scrape_custom_link(page, url, county):
    page.goto(url, timeout=60000)
    return [a.get_attribute("href") for a in page.query_selector_all("a[href$='.pdf']")]

VENDOR_FUNCS = {
    "masslandrecords": scrape_masslandrecords,
    "browntech_barnstable": scrape_browntech_barnstable,
    "browntech_alis": scrape_browntech_alis,
    "kofile_titleview": scrape_kofile_titleview,
    "kofile_taunton": scrape_kofile_taunton,
    "custom_link": scrape_custom_link
}

def run_one(county):
    vendor = county.get("vendor")
    url = county.get("search_url")
    name = county.get("name")
    func = VENDOR_FUNCS.get(vendor, scrape_custom_link)
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        try:
            links = func(page, url, name) or []
        except Exception as e:
            print(f"[{name}] error: {e}")
            links = []
        browser.close()
    return [{"county": name, "pdf_url": href} for href in dict.fromkeys(links) if href]

def main():
    ensure_dirs()
    counties = load_counties()
    # For a fast first test, uncomment next line to run Suffolk only:
    # counties = [c for c in counties if c["name"] == "Suffolk"]
    all_rows = []
    for c in counties:
        all_rows.extend(run_one(c))
    df = pd.DataFrame(all_rows)
    out = Path(OUT_CSV)
    if out.exists():
        old = pd.read_csv(out)
        df = pd.concat([old, df], ignore_index=True).drop_duplicates(subset=["county","pdf_url"])
    df.to_csv(out, index=False)
    print(f"Saved {len(df)} rows to {out}")

if __name__ == "__main__":
    main()
